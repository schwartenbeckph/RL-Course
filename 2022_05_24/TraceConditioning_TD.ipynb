{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TraceConditioning_TD.ipynb","provenance":[],"toc_visible":true,"collapsed_sections":[],"authorship_tag":"ABX9TyMIyLNCNK6orIgJkktc+vud"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# TD Learning Basics"],"metadata":{"id":"jy1UcAj0JfSE"}},{"cell_type":"markdown","source":["In the previous notebook, we've seem that we can account for simple forms of learning using variants of **Rescorla-Wagner learning**.\n","\n","However, this learning model is very **limited** to simple cases of **delay conditioning**, where the CS is still present when the US arrives.\n","\n","**Temporal Difference (TD) learning** is a very powerful framework that incorporates insights from both learning theory and optimal control. \n","\n","As we see below, TD-learning allows to **backpropagate** value estimates through time, and thus account for **trace conditioning** problems (where the CS is off before the US appears). In the same way, TD-learning can account for higher-order conditioning, where a predictive relationship between a second CS and the first CS is established, such that the reward is already predicted by the new CS.\n","\n","This notebook guides through the basic principles of TD-learning. The structure of the code is inspired by the excellent TD learning tutorial from Neuromatch 2020 (https://academy.neuromatch.io/nma2020/course-materials)."],"metadata":{"id":"g6Zqxxr6fnw1"}},{"cell_type":"markdown","source":["We start with the usual imports and defining some helper functions."],"metadata":{"id":"oaYa8pGiHCUY"}},{"cell_type":"code","execution_count":22,"metadata":{"id":"TKHaHwElccdW","executionInfo":{"status":"ok","timestamp":1652170037411,"user_tz":-120,"elapsed":743,"user":{"displayName":"Philipp Schwartenbeck","userId":"16328622913884312255"}}},"outputs":[],"source":["# Imports\n","import numpy as np                 \n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["# @title Helper functions to plot\n","\n","def plot_value_function(V, CS=None, US=None):\n","  # Plot value estimates\n","\n","  plt.stem(V, use_line_collection=True) # stem plot (vertical lines at x location from baseline to y, and place marker there)\n","  plt.ylabel('Value')\n","  plt.xlabel('State')\n","  plt.title(\"Value function: $V(s)$\")\n","\n","  if (not CS and not US):\n","    x_tick_vals = np.linspace(0,len(V),5).astype(int)\n","    x_tick_lab = x_tick_vals\n","  else:\n","    x_tick_vals = [0,CS,US,len(V)]\n","    x_tick_lab  = [0,\"CS\",\"US\",len(V)]\n","\n","  plt.xticks(x_tick_vals,x_tick_lab)\n","  plt.show()\n","\n","def plot_tde_trace(TDE, skip=400):\n","  # Plot TD Error\n","\n","  indx = np.arange(0, TDE.shape[1], skip)\n","  im = plt.imshow(TDE[:,indx])\n","\n","  plt.xticks([0,10,20,30,40],np.linspace(0,np.size(TDE,1),5).astype(int))\n","  plt.yticks([0,10,20,30])\n","\n","  plt.title('TD-error over learning')\n","  plt.ylabel('State')\n","  plt.xlabel('Iterations')\n","  plt.colorbar(im)\n","  \n","  plt.show()"],"metadata":{"id":"uopzPHUPf651","executionInfo":{"status":"ok","timestamp":1652170038283,"user_tz":-120,"elapsed":6,"user":{"displayName":"Philipp Schwartenbeck","userId":"16328622913884312255"}},"cellView":"form"},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["TD learning modifies the way value is learned in a small but decisive way:\n","\n","$$V_t \\leftarrow V_t + \\alpha \\cdot (r+\\gamma \\cdot V_{t+1}-V_t) \\tag{1} $$ \n","\n","There are now two key changes:\n","\n","\n","*   We now deal with **real time $t$**, depending on some arbitrary temporal resolution of a problem\n","*   We include the term $\\gamma \\cdot V_{t+1}$ in the prediciton error term. Think of this as an **additional reward**. This gives an agent the sense of not only the **immediate reward but also the reward that is likely to follow** in the future.\n","\n","The change to include $\\gamma \\cdot V_{t+1}$ seems small, but it has substantial consequences in how reward information can be learnt and **back-propagated** in time.\n","\n","$\\gamma$ is a **discount parameter**, and roughly translates into how much you value future reward in relation to immediate reward. Consequently, $\\gamma$ is reasonably bounded between [0,1] and is also an important parameter to account for delay discounting tendencies in individual behaviour.\n","\n","In general, the TD learning framework is highly flexible and often serves as a (reasonable) benchmark even in complex learning problems.\n","\n"],"metadata":{"id":"XovlhlogVkrr"}},{"cell_type":"markdown","source":["---\n","# Part 1: TD Algorithm"],"metadata":{"id":"kgwOoFh6gf5F"}},{"cell_type":"markdown","source":["Let's define a simple application: a trial has 40 time-steps, and an animal receives an unconditioned stimulus (US) (i.e. a primary reward, such as juice drops) at time-step 20. However, we deliver a conditioned stimulus (CS) after 10 trials (i.e., at time-step 10 since we start counting at 0). \n","\n","How does value learning under a TD model look like in such a task?"],"metadata":{"id":"Yd7qw04a5EPF"}},{"cell_type":"code","source":["def td_learner(n_trials=1000, n_steps=40, gamma=0.98, alpha=0.2):\n","  \"\"\" Temporal Difference learning\n","\n","  Input:\n","    n_trials (int): the number of trials to run\n","    n_steps (int): length of a trial (= temporal resolution)\n","    gamma (float): temporal discount factor\n","    alpha (float): learning rate\n","  \n","  Returns:\n","    ndarray, ndarray: the value function and temporal difference error arrays\n","  \"\"\"\n","\n","  V = np.zeros(n_steps) # Array to store values over states (time)\n","  TDE = np.zeros((n_steps, n_trials)) # Array to store TD errors\n","\n","  for n in np.arange(n_trials):\n","\n","    state = 0 # Initial state\n","\n","    for t in np.arange(n_steps):\n","\n","      # Get next state and next reward\n","      if state < n_steps - 1: \n","        next_state = state + 1 # increase state if trial not finished\n","      else:\n","        next_state = 0 # reset state to initial state otherwise (this can account for an interesting effect, see below)\n","\n","      if state == 20:\n","        reward = 1\n","      else:\n","        reward = 0\n","\n","      # Is the current state in the delay period (after CS)?\n","      if (state<=20 and state>10):\n","        is_delay = 1\n","      else:\n","        is_delay = 0\n","\n","      # Write an expression to compute the TD-error\n","      TDE[state, n] = reward + gamma * V[next_state] - V[state]\n","\n","      # Write an expression to update the value function\n","      V[state] += alpha * TDE[state, n] * is_delay\n","\n","      # Update state\n","      state = next_state\n","  \n","  return V, TDE"],"metadata":{"id":"j_CdNYV0gRkZ","executionInfo":{"status":"ok","timestamp":1652170038283,"user_tz":-120,"elapsed":5,"user":{"displayName":"Philipp Schwartenbeck","userId":"16328622913884312255"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["V, TDE = td_learner(n_trials=20000, n_steps=40, gamma=0.98, alpha=0.001)\n","\n","plt.rcParams['figure.figsize'] = [10, 3]\n","plot_value_function(V,10,20)\n","# print(V)\n","\n","plt.rcParams['figure.figsize'] = [8, 5]\n","plot_tde_trace(TDE)\n","# print(TDE[:,-1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":576},"id":"DrXdWiSqquKD","executionInfo":{"status":"ok","timestamp":1652170041217,"user_tz":-120,"elapsed":2938,"user":{"displayName":"Philipp Schwartenbeck","userId":"16328622913884312255"}},"outputId":"65f1a587-8ba3-44d6-f86e-c53d64c70efe"},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 720x216 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"display_data","data":{"text/plain":["<Figure size 576x360 with 2 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["Note that the way we defined the algorithm implies that the CS does not have a value itself, but evokes a positive prediction error because it *predicts* the begin of the delay period to the reward."],"metadata":{"id":"9flwTp_T4iOg"}},{"cell_type":"code","source":["# @title Exercise 1.1: How does TD work?"],"metadata":{"cellView":"form","id":"G7URBrS2TAAj","executionInfo":{"status":"ok","timestamp":1652170041217,"user_tz":-120,"elapsed":9,"user":{"displayName":"Philipp Schwartenbeck","userId":"16328622913884312255"}}},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":["These are complicated plots. Do you understand what's going on?\n","\n","Why does the value of states between the CS and US gradually increase? Why is it 0 in all other states?\n","\n","Can you explain the gradual change of the TD-error over the simulations?"],"metadata":{"id":"MIP38ZEyTdNk"}},{"cell_type":"code","source":["# @title Exercise 1.2: How does the TD error work?"],"metadata":{"cellView":"form","id":"2FHvnd4sUYt0","executionInfo":{"status":"ok","timestamp":1652170041218,"user_tz":-120,"elapsed":9,"user":{"displayName":"Philipp Schwartenbeck","userId":"16328622913884312255"}}},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":["Can you work out **numerically** (use the TD error in equation 1 above):\n","\n","i) the TD error at the very first delivery of the US (row 20, first column in the second (image) plot above?)\n","\n","ii) the TD error at the very last delivery of the US (row 20, last column in the second (image) plot above?)\n","\n","iii) the TD error at the very first delivery of the CS (row 10, first column in the second (image) plot above?)\n","\n","iv) the TD error at the very last delivery of the CS (row 10, last column in the second (image) plot above?)"],"metadata":{"id":"83wD6v5oUfYo"}},{"cell_type":"markdown","source":["---\n","# Part 2: Parameters of the TD Algorithm"],"metadata":{"id":"jih-tcJiVfa-"}},{"cell_type":"code","source":["# @title Exercise 2.1: Importance of delayed rewards"],"metadata":{"cellView":"form","id":"zcuam3deYlsx","executionInfo":{"status":"ok","timestamp":1652170041218,"user_tz":-120,"elapsed":8,"user":{"displayName":"Philipp Schwartenbeck","userId":"16328622913884312255"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["What happens if the value of next time step is not included? Define a function called 'rw_learner' (or so) that does the same thing as the td_learner function above, but in a Rescorla-Wagner way. What do you observe when plotting the results?"],"metadata":{"id":"URaodcZvn2Lr"}},{"cell_type":"code","source":["# Your code goes here"],"metadata":{"id":"AA5gVJPIkUvI","executionInfo":{"status":"ok","timestamp":1652170041218,"user_tz":-120,"elapsed":8,"user":{"displayName":"Philipp Schwartenbeck","userId":"16328622913884312255"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["# @title Partial Solution\n","\n","def rw_learner(n_trials=1000, n_steps=40, gamma=0.98, alpha=0.2):\n","  \"\"\" Temporal Difference learning\n","\n","  Input:\n","    n_trials (int): the number of trials to run\n","    n_steps (int): length of a trial (= temporal resolution)\n","    gamma (float): temporal discount factor\n","    alpha (float): learning rate\n","  \n","  Returns:\n","    ndarray, ndarray: the value function and temporal difference error arrays\n","  \"\"\"\n","\n","  V = np.zeros(n_steps) # Array to store values over states (time)\n","  TDE = np.zeros((n_steps, n_trials)) # Array to store TD errors\n","\n","  for n in np.arange(n_trials):\n","\n","    state = 0 # Initial state\n","\n","    for t in np.arange(n_steps):\n","\n","      # Get next state and next reward\n","      if state < n_steps - 1: \n","        next_state = state + 1 # increase state if trial not finished\n","      else:\n","        next_state = 0 # reset state to initial state otherwise (this can account for an interesting effect, see below)\n","\n","      if state == 20:\n","        reward = 1\n","      else:\n","        reward = 0\n","\n","      # Is the current state in the delay period (after CS)?\n","      if (state<=20 and state>10):\n","        is_delay = 1\n","      else:\n","        is_delay = 0\n","\n","      # Write an expression to compute the TD-error\n","      #####\n","      # uncomment and implement the Rescorla Wagner prediction error:\n","      # TDE[state, n] = ...\n","      #####\n","\n","\n","      # Write an expression to update the value function\n","      V[state] += alpha * TDE[state, n] * is_delay\n","\n","      # Update state\n","      state = next_state\n","  \n","  return V, TDE\n","\n","V, TDE = rw_learner(n_trials=20000, n_steps=40, gamma=0.98, alpha=0.001)\n","\n","plt.rcParams['figure.figsize'] = [10, 3]\n","plot_value_function(V,10,20)\n","\n","plt.rcParams['figure.figsize'] = [8, 5]\n","plot_tde_trace(TDE)"],"metadata":{"cellView":"form","id":"yrz3eURGdhTY","executionInfo":{"status":"ok","timestamp":1652170044950,"user_tz":-120,"elapsed":269,"user":{"displayName":"Philipp Schwartenbeck","userId":"16328622913884312255"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["# @title Full Solution\n","\n","def rw_learner(n_trials=1000, n_steps=40, gamma=0.98, alpha=0.2):\n","  \"\"\" Temporal Difference learning\n","\n","  Input:\n","    n_trials (int): the number of trials to run\n","    n_steps (int): length of a trial (= temporal resolution)\n","    gamma (float): temporal discount factor\n","    alpha (float): learning rate\n","  \n","  Returns:\n","    ndarray, ndarray: the value function and temporal difference error arrays\n","  \"\"\"\n","\n","  V = np.zeros(n_steps) # Array to store values over states (time)\n","  TDE = np.zeros((n_steps, n_trials)) # Array to store TD errors\n","\n","  for n in np.arange(n_trials):\n","\n","    state = 0 # Initial state\n","\n","    for t in np.arange(n_steps):\n","\n","      # Get next state and next reward\n","      if state < n_steps - 1: \n","        next_state = state + 1 # increase state if trial not finished\n","      else:\n","        next_state = 0 # reset state to initial state otherwise (this can account for an interesting effect, see below)\n","\n","      if state == 20:\n","        reward = 1\n","      else:\n","        reward = 0\n","\n","      # Is the current state in the delay period (after CS)?\n","      if (state<=20 and state>10):\n","        is_delay = 1\n","      else:\n","        is_delay = 0\n","\n","      # Write an expression to compute the TD-error\n","      TDE[state, n] = reward  - V[state]\n","\n","      # Write an expression to update the value function\n","      V[state] += alpha * TDE[state, n] * is_delay\n","\n","      # Update state\n","      state = next_state\n","  \n","  return V, TDE\n","\n","print(\"Output should look like this:\")\n","\n","V, TDE = rw_learner(n_trials=20000, n_steps=40, gamma=0.98, alpha=0.001)\n","\n","plt.rcParams['figure.figsize'] = [10, 3]\n","plot_value_function(V,10,20)\n","\n","plt.rcParams['figure.figsize'] = [8, 5]\n","plot_tde_trace(TDE)"],"metadata":{"cellView":"form","id":"UII9s_4vc-Xt","executionInfo":{"status":"ok","timestamp":1652170056991,"user_tz":-120,"elapsed":288,"user":{"displayName":"Philipp Schwartenbeck","userId":"16328622913884312255"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["# @title Exercise 2.2: Importance of $\\gamma$"],"metadata":{"cellView":"form","id":"9sktnst1ZG-8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["What's the role of $\\gamma$? What do the results look like with a smaller $\\gamma$, or when you set $\\gamma$ to 0 or 1?"],"metadata":{"id":"TTRVONlSZUh4"}},{"cell_type":"code","source":["# Your code goes here"],"metadata":{"id":"IRwcJUcWkXH_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Full Solution\n","\n","print(\"Output should look like this:\")\n","\n","# i) low gamma:\n","print(\"Low gamma:\")\n","V, TDE = td_learner(n_trials=20000, n_steps=40, gamma=0.60, alpha=0.001)\n","plt.rcParams['figure.figsize'] = [10, 3]\n","plot_value_function(V,10,20)\n","\n","plt.rcParams['figure.figsize'] = [8, 5]\n","plot_tde_trace(TDE)\n","\n","# i) Gamma=1:\n","print(\"Gamma=1:\")\n","V, TDE = td_learner(n_trials=20000, n_steps=40, gamma=1, alpha=0.001)\n","plt.rcParams['figure.figsize'] = [10, 3]\n","plot_value_function(V,10,20)\n","\n","plt.rcParams['figure.figsize'] = [8, 5]\n","plot_tde_trace(TDE)\n","\n","# i) Gamma=0:\n","print(\"Gamma=0:\")\n","V, TDE = td_learner(n_trials=20000, n_steps=40, gamma=0, alpha=0.001)\n","plt.rcParams['figure.figsize'] = [10, 3]\n","plot_value_function(V,10,20)\n","\n","plt.rcParams['figure.figsize'] = [8, 5]\n","plot_tde_trace(TDE)"],"metadata":{"cellView":"form","id":"9k7eqZP0eB8L","executionInfo":{"status":"ok","timestamp":1652170075175,"user_tz":-120,"elapsed":242,"user":{"displayName":"Philipp Schwartenbeck","userId":"16328622913884312255"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["# @title Exercise 2.3: importance of $\\alpha$"],"metadata":{"cellView":"form","id":"mJVBENAkZPCU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["How do the results change under different learning rates $\\alpha$?"],"metadata":{"id":"e_E2FYBFZVej"}},{"cell_type":"code","source":["# Your code goes here"],"metadata":{"id":"4NrphYbUkZKL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Full Solution\n","\n","print(\"Output should look like this:\")\n","\n","# i) Low alpha:\n","print(\"Low alpha:\")\n","V, TDE = td_learner(n_trials=20000, n_steps=40, gamma=0.98, alpha=0.0001)\n","plt.rcParams['figure.figsize'] = [10, 3]\n","plot_value_function(V,10,20)\n","\n","plt.rcParams['figure.figsize'] = [8, 5]\n","plot_tde_trace(TDE)\n","\n","# i) High alpha:\n","print(\"High alpha:\")\n","V, TDE = td_learner(n_trials=20000, n_steps=40, gamma=0.98, alpha=0.1)\n","plt.rcParams['figure.figsize'] = [10, 3]\n","plot_value_function(V,10,20)\n","\n","plt.rcParams['figure.figsize'] = [8, 5]\n","plot_tde_trace(TDE)"],"metadata":{"cellView":"form","id":"Hb3Rz9z0e7iL","executionInfo":{"status":"ok","timestamp":1652170084157,"user_tz":-120,"elapsed":329,"user":{"displayName":"Philipp Schwartenbeck","userId":"16328622913884312255"}}},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":["---\n","# Part 3: Different reward values"],"metadata":{"id":"xoI806rcZ4rE"}},{"cell_type":"code","source":["# @title Exercise 3.1: Multiple reward values"],"metadata":{"cellView":"form","id":"dVDogOlif_Bd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Implement a function that applies TD-learning, but now the reward is not always consistent. Let's start with situations in which there are different reward values - but the agent ALWAYS obtains some positive reward (note that this is different from the reward sampling in the previous notebook).\n","\n","Specifically, try a case like reward_vals = [0.6,1.4] sampled with 50% each - i.e. a case where there are several reward values, but the expected value is the same as in the consistent reward case."],"metadata":{"id":"h7Q6jRBczeZu"}},{"cell_type":"code","source":["# Your code goes here"],"metadata":{"id":"XcUx2fR6kaWf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Partial Solution\n","\n","def td_learner_multR(n_trials=1000, n_steps=40, gamma=0.98, alpha=0.2):\n","  \"\"\" Temporal Difference learning\n","\n","  Input:\n","    n_trials (int): the number of trials to run\n","    n_steps (int): length of a trial (= temporal resolution)\n","    gamma (float): temporal discount factor\n","    alpha (float): learning rate\n","  \n","  Returns:\n","    ndarray, ndarray: the value function and temporal difference error arrays\n","  \"\"\"\n","\n","  V = np.zeros(n_steps) # Array to store values over states (time)\n","  TDE = np.zeros((n_steps, n_trials)) # Array to store TD errors\n","\n","  for n in np.arange(n_trials):\n","\n","    state = 0 # Initial state\n","\n","    for t in np.arange(n_steps):\n","\n","      # Get next state and next reward\n","      if state < n_steps - 1: \n","        next_state = state + 1 # increase state if trial not finished\n","      else:\n","        next_state = 0 # reset state to initial state otherwise (this can account for an interesting effect, see below)\n","\n","      if state == 20:\n","        #####\n","        reward = 0 # comment this line\n","        # uncomment and implement sampling from multiple reward values:\n","        # reward = ... \n","        #####\n","      else:\n","        reward = 0\n","\n","      # Is the current state in the delay period (after CS)?\n","      if (state<=20 and state>10):\n","        is_delay = 1\n","      else:\n","        is_delay = 0\n","\n","      # Write an expression to compute the TD-error\n","      TDE[state, n] = reward + gamma * V[next_state] - V[state]\n","\n","      # Write an expression to update the value function\n","      V[state] += alpha * TDE[state, n] * is_delay\n","\n","      # Update state\n","      state = next_state\n","  \n","  return V, TDE\n","\n","V, TDE = td_learner_multR(n_trials=20000, n_steps=40, gamma=0.98, alpha=0.001)\n","\n","plt.rcParams['figure.figsize'] = [10, 3]\n","plot_value_function(V,10,20)\n","\n","plt.rcParams['figure.figsize'] = [8, 5]\n","plot_tde_trace(TDE)"],"metadata":{"cellView":"form","id":"lQMFcXeRhe-X","executionInfo":{"status":"ok","timestamp":1652170094682,"user_tz":-120,"elapsed":268,"user":{"displayName":"Philipp Schwartenbeck","userId":"16328622913884312255"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["# @title Full Solution\n","\n","def td_learner_multR(n_trials=1000, n_steps=40, gamma=0.98, alpha=0.2):\n","  \"\"\" Temporal Difference learning\n","\n","  Input:\n","    n_trials (int): the number of trials to run\n","    n_steps (int): length of a trial (= temporal resolution)\n","    gamma (float): temporal discount factor\n","    alpha (float): learning rate\n","  \n","  Returns:\n","    ndarray, ndarray: the value function and temporal difference error arrays\n","  \"\"\"\n","\n","  V = np.zeros(n_steps) # Array to store values over states (time)\n","  TDE = np.zeros((n_steps, n_trials)) # Array to store TD errors\n","\n","  for n in np.arange(n_trials):\n","\n","    state = 0 # Initial state\n","\n","    for t in np.arange(n_steps):\n","\n","      # Get next state and next reward\n","      if state < n_steps - 1: \n","        next_state = state + 1 # increase state if trial not finished\n","      else:\n","        next_state = 0 # reset state to initial state otherwise (this can account for an interesting effect, see below)\n","\n","      if state == 20:\n","        reward=np.random.choice([0.6,1.4])\n","      else:\n","        reward = 0\n","\n","      # Is the current state in the delay period (after CS)?\n","      if (state<=20 and state>10):\n","        is_delay = 1\n","      else:\n","        is_delay = 0\n","\n","      # Write an expression to compute the TD-error\n","      TDE[state, n] = reward + gamma * V[next_state] - V[state]\n","\n","      # Write an expression to update the value function\n","      V[state] += alpha * TDE[state, n] * is_delay\n","\n","      # Update state\n","      state = next_state\n","  \n","  return V, TDE\n","\n","V, TDE = td_learner_multR(n_trials=20000, n_steps=40, gamma=0.98, alpha=0.001)\n","\n","print(\"Output should look like this:\")\n","\n","plt.rcParams['figure.figsize'] = [10, 3]\n","plot_value_function(V,10,20)\n","\n","plt.rcParams['figure.figsize'] = [8, 5]\n","plot_tde_trace(TDE)"],"metadata":{"cellView":"form","id":"hYUPMnZHg-m_","executionInfo":{"status":"ok","timestamp":1652170104523,"user_tz":-120,"elapsed":316,"user":{"displayName":"Philipp Schwartenbeck","userId":"16328622913884312255"}}},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":["If implemented correctly, you should now see that there are also negative TD errors. Why?"],"metadata":{"id":"e4Ame0DOhRmn"}},{"cell_type":"markdown","source":["---\n","# Part 4: Probabilistic reward"],"metadata":{"id":"0nlG9G2aZ-ip"}},{"cell_type":"code","source":["# @title Exercise 4.1: Stochastic reward delivery"],"metadata":{"cellView":"form","id":"2z30IQrHiclV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, implement a function that delivers probabilistic reward, i.e. some positive reward with a certain probability or no reward (reward = 0) with some other probability."],"metadata":{"id":"EiwS3sjYijC5"}},{"cell_type":"code","source":["# Your code goes here"],"metadata":{"id":"bqSCpKOykc97"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Partial Solution\n","\n","def td_learner_probR(n_trials=1000, n_steps=40, gamma=0.98, alpha=0.2):\n","  \"\"\" Temporal Difference learning\n","\n","  Input:\n","    n_trials (int): the number of trials to run\n","    n_steps (int): length of a trial (= temporal resolution)\n","    gamma (float): temporal discount factor\n","    alpha (float): learning rate\n","  \n","  Returns:\n","    ndarray, ndarray: the value function and temporal difference error arrays\n","  \"\"\"\n","\n","  V = np.zeros(n_steps) # Array to store values over states (time)\n","  TDE = np.zeros((n_steps, n_trials)) # Array to store TD errors\n","\n","  for n in np.arange(n_trials):\n","\n","    state = 0 # Initial state\n","\n","    for t in np.arange(n_steps):\n","\n","      # Get next state and next reward\n","      if state < n_steps - 1: \n","        next_state = state + 1 # increase state if trial not finished\n","      else:\n","        next_state = 0 # reset state to initial state otherwise (this can account for an interesting effect, see below)\n","\n","      if state == 20:\n","        #####\n","        reward = 0 # comment this line\n","        # uncomment and implement sampling from multiple reward values:\n","        # reward = ... \n","        #####\n","      else:\n","        reward = 0\n","\n","      # Is the current state in the delay period (after CS)?\n","      if (state<=20 and state>10):\n","        is_delay = 1\n","      else:\n","        is_delay = 0\n","\n","      # Write an expression to compute the TD-error\n","      TDE[state, n] = reward + gamma * V[next_state] - V[state]\n","\n","      # Write an expression to update the value function\n","      V[state] += alpha * TDE[state, n] * is_delay\n","\n","      # Update state\n","      state = next_state\n","  \n","  return V, TDE\n","\n","V, TDE = td_learner_probR(n_trials=20000, n_steps=40, gamma=0.98, alpha=0.001)\n","\n","plt.rcParams['figure.figsize'] = [10, 3]\n","plot_value_function(V,10,20)\n","\n","plt.rcParams['figure.figsize'] = [8, 5]\n","plot_tde_trace(TDE)"],"metadata":{"cellView":"form","id":"93uuDN2qjyG-","executionInfo":{"status":"ok","timestamp":1652170115952,"user_tz":-120,"elapsed":282,"user":{"displayName":"Philipp Schwartenbeck","userId":"16328622913884312255"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["# @title Full Solution\n","\n","def td_learner_probR(n_trials=1000, n_steps=40, gamma=0.98, alpha=0.2):\n","  \"\"\" Temporal Difference learning\n","\n","  Input:\n","    n_trials (int): the number of trials to run\n","    n_steps (int): length of a trial (= temporal resolution)\n","    gamma (float): temporal discount factor\n","    alpha (float): learning rate\n","  \n","  Returns:\n","    ndarray, ndarray: the value function and temporal difference error arrays\n","  \"\"\"\n","\n","  V = np.zeros(n_steps) # Array to store values over states (time)\n","  TDE = np.zeros((n_steps, n_trials)) # Array to store TD errors\n","\n","  for n in np.arange(n_trials):\n","\n","    state = 0 # Initial state\n","\n","    for t in np.arange(n_steps):\n","\n","      # Get next state and next reward\n","      if state < n_steps - 1: \n","        next_state = state + 1 # increase state if trial not finished\n","      else:\n","        next_state = 0 # reset state to initial state otherwise (this can account for an interesting effect, see below)\n","\n","      if state == 20:\n","        reward=np.random.choice([0,1])\n","      else:\n","        reward = 0\n","\n","      # Is the current state in the delay period (after CS)?\n","      if (state<=20 and state>10):\n","        is_delay = 1\n","      else:\n","        is_delay = 0\n","\n","      # Write an expression to compute the TD-error\n","      TDE[state, n] = reward + gamma * V[next_state] - V[state]\n","\n","      # Write an expression to update the value function\n","      V[state] += alpha * TDE[state, n] * is_delay\n","\n","      # Update state\n","      state = next_state\n","  \n","  return V, TDE\n","\n","V, TDE = td_learner_probR(n_trials=20000, n_steps=40, gamma=0.98, alpha=0.001)\n","\n","print(\"Output should look like this:\")\n","\n","plt.rcParams['figure.figsize'] = [10, 3]\n","plot_value_function(V,10,20)\n","\n","plt.rcParams['figure.figsize'] = [8, 5]\n","plot_tde_trace(TDE)"],"metadata":{"cellView":"form","id":"6j7kkENrjVTR","executionInfo":{"status":"ok","timestamp":1652170132170,"user_tz":-120,"elapsed":251,"user":{"displayName":"Philipp Schwartenbeck","userId":"16328622913884312255"}}},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":["How is this different or similar to the multiple reward value case?"],"metadata":{"id":"uIAW4auyjq-O"}},{"cell_type":"code","source":["# @title Exercise 4.2: Learning rate in stochastic reward delivery"],"metadata":{"cellView":"form","id":"NZ7FMuCNi_jm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["What happens if you use a very high learning rate (e.g. $\\alpha$ = 1) when there is probabilistic reward?"],"metadata":{"id":"PynEkpf81KIe"}},{"cell_type":"code","source":["# Your code goes here"],"metadata":{"id":"YwG9w5yBkeRj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Full Solution\n","V, TDE = td_learner_probR(n_trials=20000, n_steps=40, gamma=0.98, alpha=1)\n","\n","print(\"Output should look like this:\")\n","\n","plt.rcParams['figure.figsize'] = [10, 3]\n","plot_value_function(V,10,20)\n","\n","plt.rcParams['figure.figsize'] = [8, 5]\n","plot_tde_trace(TDE)"],"metadata":{"cellView":"form","id":"LKtvXB9GkISw","executionInfo":{"status":"ok","timestamp":1652170140908,"user_tz":-120,"elapsed":264,"user":{"displayName":"Philipp Schwartenbeck","userId":"16328622913884312255"}}},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":["---\n","# Bonus: How to define the CS-US period"],"metadata":{"id":"TYk4_X7uaFHK"}},{"cell_type":"markdown","source":["These simulations rely on a 'hack' that restricts value learning and back-propagation to the period between CS-US. What happens when you relax that restrition?\n","\n","You can check this by defining a new function of a td learner where the variable is_delay is always 1.\n","\n","The output looks quite different. Can you see what's going on? How might this relate to real animal behaviour in the lab?"],"metadata":{"id":"esw78KU62Mso"}},{"cell_type":"code","source":["# Your code goes here"],"metadata":{"id":"0J7K3cI_lSdH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Full Solution\n","\n","def td_learner_Norestrict(n_trials=1000, n_steps=40, gamma=0.98, alpha=0.2):\n","  \"\"\" Temporal Difference learning\n","\n","  Input:\n","    n_trials (int): the number of trials to run\n","    n_steps (int): length of a trial (= temporal resolution)\n","    gamma (float): temporal discount factor\n","    alpha (float): learning rate\n","  \n","  Returns:\n","    ndarray, ndarray: the value function and temporal difference error arrays\n","  \"\"\"\n","\n","  V = np.zeros(n_steps) # Array to store values over states (time)\n","  TDE = np.zeros((n_steps, n_trials)) # Array to store TD errors\n","\n","  for n in np.arange(n_trials):\n","\n","    state = 0 # Initial state\n","\n","    for t in np.arange(n_steps):\n","\n","      # Get next state and next reward\n","      if state < n_steps - 1: \n","        next_state = state + 1 # increase state if trial not finished\n","      else:\n","        next_state = 0 # reset state to initial state otherwise (this can account for an interesting effect, see below)\n","\n","      if state == 20:\n","        reward = 1\n","      else:\n","        reward = 0\n","\n","      # Is the current state in the delay period (after CS)?\n","      if (state<=20 and state>10):\n","        is_delay = 1\n","      else:\n","        is_delay = 1\n","\n","      # Write an expression to compute the TD-error\n","      TDE[state, n] = reward + gamma * V[next_state] - V[state]\n","\n","      # Write an expression to update the value function\n","      V[state] += alpha * TDE[state, n] * is_delay\n","\n","      # Update state\n","      state = next_state\n","  \n","  return V, TDE\n","\n","V, TDE = td_learner_Norestrict(n_trials=20000, n_steps=40, gamma=0.98, alpha=0.001)\n","\n","print(\"Output should look like this:\")\n","\n","plt.rcParams['figure.figsize'] = [10, 3]\n","plot_value_function(V,10,20)\n","\n","plt.rcParams['figure.figsize'] = [8, 5]\n","plot_tde_trace(TDE)"],"metadata":{"cellView":"form","id":"Eg7irPpsk4n_","executionInfo":{"status":"ok","timestamp":1652170151653,"user_tz":-120,"elapsed":341,"user":{"displayName":"Philipp Schwartenbeck","userId":"16328622913884312255"}}},"execution_count":39,"outputs":[]}]}